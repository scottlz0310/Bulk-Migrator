# ğŸ“ˆ OneDriveâ†’SharePointç§»è¡Œã‚·ã‚¹ãƒ†ãƒ  æ”¹å–„è¨ˆç”»æ›¸ 2025

**ä½œæˆæ—¥**: 2025å¹´6æœˆ30æ—¥  
**å¯¾è±¡**: OneDrive to SharePoint Bulk Migration Tool  
**ç›®çš„**: ã‚·ã‚¹ãƒ†ãƒ ã®å®‰å®šæ€§ãƒ»åŠ¹ç‡æ€§ãƒ»é‹ç”¨æ€§ã®å‘ä¸Š

---

## ğŸ¯ æ”¹å–„ã®èƒŒæ™¯ã¨ç›®æ¨™

### ç¾çŠ¶è©•ä¾¡
- âœ… **ãƒ†ã‚¹ãƒˆè‰¯å¥½**: åŸºæœ¬æ©Ÿèƒ½ã¯æ­£å¸¸å‹•ä½œ
- âœ… **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å„ªç§€**: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†é›¢ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾å¿œæ¸ˆã¿
- âœ… **é‹ç”¨æº–å‚™å®Œäº†**: å¤œé–“ãƒãƒƒãƒãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™æ¸ˆã¿

### æ”¹å–„ç›®æ¨™
- ğŸš€ **å®‰å®šæ€§å‘ä¸Š**: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»å¾©æ—§æ©Ÿèƒ½ã®å¼·åŒ–
- âš¡ **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**: å‡¦ç†åŠ¹ç‡ãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®å‘ä¸Š
- ğŸ“Š **é‹ç”¨æ€§å¼·åŒ–**: ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ€§ã®å‘ä¸Š
- ğŸ§ª **å“è³ªä¿è¨¼**: ãƒ†ã‚¹ãƒˆå¼·åŒ–ãƒ»ã‚³ãƒ¼ãƒ‰å“è³ªå‘ä¸Š

---

## ğŸ” ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ å¼·ã¿

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å„ªç§€æ€§
- **é©åˆ‡ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†é›¢**: èªè¨¼ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œãƒ»åˆ¶å¾¡ãƒ»è¨­å®šç®¡ç†
- **å …ç‰¢ãªé‹ç”¨è¨­è¨ˆ**: æ’ä»–åˆ¶å¾¡ãƒ»ãƒ­ã‚°ç®¡ç†ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾å¿œ
- **åŒ…æ‹¬çš„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: é‹ç”¨è¨ˆç”»ãƒ»æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆãƒ»ã‚¬ã‚¤ãƒ‰é¡

### å®Ÿè£…æ¸ˆã¿æ©Ÿèƒ½
- **å†ªç­‰æ€§å‡¦ç†**: ä¸­æ–­ãƒ»å†é–‹å¯èƒ½ãªè¨­è¨ˆ
- **è€éšœå®³æ€§**: è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ãƒ»ã‚¨ãƒ©ãƒ¼å›å¾©
- **å¤§å®¹é‡å¯¾å¿œ**: ãƒãƒ£ãƒ³ã‚¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»ä¸¦åˆ—å‡¦ç†
- **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: ç’°å¢ƒå¤‰æ•°ãƒ™ãƒ¼ã‚¹èªè¨¼

---

## ğŸ“‹ æ”¹å–„é …ç›®è©³ç´°

### 1. ğŸ”§ ã‚³ãƒ¼ãƒ‰å“è³ªå‘ä¸Š

#### A. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ¨™æº–åŒ–
**ç¾çŠ¶**: å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å€‹åˆ¥ã®ã‚¨ãƒ©ãƒ¼å‡¦ç†  
**æ”¹å–„**: çµ±ä¸€ä¾‹å¤–ã‚¯ãƒ©ã‚¹ãƒ»æ¨™æº–ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®å°å…¥

```python
# æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: exceptions.py
class BatchError(Exception):
    """ãƒãƒƒãƒå‡¦ç†å°‚ç”¨ä¾‹å¤–åŸºåº•ã‚¯ãƒ©ã‚¹"""
    def __init__(self, message, error_code=None, retry_after=None):
        super().__init__(message)
        self.error_code = error_code
        self.retry_after = retry_after
        self.timestamp = datetime.now()

class RetryableError(BatchError):
    """ãƒªãƒˆãƒ©ã‚¤å¯èƒ½ã‚¨ãƒ©ãƒ¼ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ»ä¸€æ™‚çš„éšœå®³ï¼‰"""
    pass

class FatalError(BatchError):
    """è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ï¼ˆãƒªãƒˆãƒ©ã‚¤ä¸å¯ãƒ»è¨­å®šä¸å‚™ãªã©ï¼‰"""
    pass

class RateLimitError(RetryableError):
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ï¼ˆGraph APIåˆ¶é™ï¼‰"""
    pass
```

**å®Ÿè£…å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«**:
- `auth_helper.py` - èªè¨¼ã‚¨ãƒ©ãƒ¼ã®æ¨™æº–åŒ–
- `graph_api_helper.py` - API ã‚¨ãƒ©ãƒ¼ã®çµ±ä¸€å‡¦ç†
- `file_downloader.py` - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ã®åˆ†é¡
- `sharepoint_uploader.py` - ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ã®åˆ†é¡

#### B. è¨­å®šæ¤œè¨¼ã®å¼·åŒ–
**ç¾çŠ¶**: åŸºæœ¬çš„ãªè¨­å®šå€¤ãƒã‚§ãƒƒã‚¯ã®ã¿  
**æ”¹å–„**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦³ç‚¹ã®æ¤œè¨¼è¿½åŠ 

```python
# batch_config.py ã¸ã®è¿½åŠ æ©Ÿèƒ½
class ConfigValidator:
    def validate_performance_settings(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®šå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        workers = self.config.get("batch_settings", {})
        
        # SharePointåˆ¶é™ã‚’è€ƒæ…®ã—ãŸä¸¦åˆ—æ•°ãƒã‚§ãƒƒã‚¯
        if workers.get("max_workers_upload", 2) > 4:
            self.logger.warning("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸¦åˆ—æ•°éå¤š: SharePointåˆ¶é™è€ƒæ…®ãŒå¿…è¦")
            
        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        chunk_size = self.config.get("max_chunk_size_mb", 4)
        if chunk_size > 60:  # SharePointä¸Šé™
            raise ValueError("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãŒSharePointä¸Šé™(60MB)ã‚’è¶…é")
            
    def validate_security_settings(self):
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šãƒã‚§ãƒƒã‚¯"""
        # ç’°å¢ƒå¤‰æ•°ã®å­˜åœ¨ç¢ºèª
        required_env = ["CLIENT_ID", "TENANT_ID", "CLIENT_SECRET"]
        missing = [env for env in required_env if not os.getenv(env)]
        if missing:
            raise ValueError(f"å¿…é ˆç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®š: {missing}")
```

### 2. âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

#### A. å‹•çš„ä¸¦åˆ—åº¦èª¿æ•´
**ç¾çŠ¶**: å›ºå®šã®ä¸¦åˆ—æ•°è¨­å®š  
**æ”¹å–„**: ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹æ€§ã«å¿œã˜ãŸè‡ªå‹•æœ€é©åŒ–

```python
# æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: performance_optimizer.py
class PerformanceOptimizer:
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
    def optimize_workers(self, files):
        """ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹æ€§ã«å¿œã˜ãŸä¸¦åˆ—åº¦æœ€é©åŒ–"""
        if not files:
            return {"download": 2, "upload": 1}
            
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ†æ
        sizes = [f.get("size", 0) for f in files if isinstance(f, dict)]
        if not sizes:
            return {"download": 2, "upload": 1}
            
        avg_size = sum(sizes) / len(sizes)
        total_size = sum(sizes)
        
        # ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–
        if avg_size > 50 * 1024 * 1024:  # 50MBä»¥ä¸Š
            workers = {"download": 1, "upload": 1}  # å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«
        elif avg_size > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
            workers = {"download": 2, "upload": 1}  # ä¸­å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«
        elif total_size > 1024 * 1024 * 1024:  # ç·å®¹é‡1GBä»¥ä¸Š
            workers = {"download": 6, "upload": 3}  # å¤§é‡å°ãƒ•ã‚¡ã‚¤ãƒ«
        else:
            workers = {"download": 4, "upload": 2}  # æ¨™æº–
            
        self.logger.info(f"ä¸¦åˆ—åº¦æœ€é©åŒ–: {workers} (å¹³å‡ã‚µã‚¤ã‚º: {avg_size/1024/1024:.1f}MB)")
        return workers
        
    def optimize_chunk_size(self, file_size):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ãŸãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºæœ€é©åŒ–"""
        if file_size > 100 * 1024 * 1024:  # 100MBä»¥ä¸Š
            return 8  # 8MB chunk
        elif file_size > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
            return 4  # 4MB chunk
        else:
            return 2  # 2MB chunk
```

#### B. ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ç›£è¦–ã®å¼·åŒ–
**ç¾çŠ¶**: åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚«ã‚¦ãƒ³ãƒˆã®ã¿  
**æ”¹å–„**: è©³ç´°é€²æ—ãƒ»å®Œäº†äºˆæ¸¬ãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆç›£è¦–

```python
# æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: progress_monitor.py
class ProgressMonitor:
    def __init__(self):
        self.start_time = datetime.now()
        self.processed_files = 0
        self.processed_bytes = 0
        self.total_files = 0
        self.total_bytes = 0
        self.error_count = 0
        self.phase_times = {}
        
    def start_phase(self, phase_name):
        """ãƒ•ã‚§ãƒ¼ã‚ºé–‹å§‹è¨˜éŒ²"""
        self.phase_times[phase_name] = {"start": datetime.now()}
        
    def end_phase(self, phase_name):
        """ãƒ•ã‚§ãƒ¼ã‚ºçµ‚äº†è¨˜éŒ²"""
        if phase_name in self.phase_times:
            self.phase_times[phase_name]["end"] = datetime.now()
            duration = self.phase_times[phase_name]["end"] - self.phase_times[phase_name]["start"]
            self.phase_times[phase_name]["duration"] = duration.total_seconds()
            
    def update_progress(self, files_processed=0, bytes_processed=0, errors=0):
        """é€²æ—æ›´æ–°"""
        self.processed_files += files_processed
        self.processed_bytes += bytes_processed
        self.error_count += errors
        
    def estimate_completion(self):
        """å®Œäº†äºˆæƒ³æ™‚åˆ»è¨ˆç®—"""
        if self.processed_files == 0:
            return None
            
        elapsed = datetime.now() - self.start_time
        rate = self.processed_files / elapsed.total_seconds()
        
        if rate > 0:
            remaining_files = self.total_files - self.processed_files
            remaining_seconds = remaining_files / rate
            return datetime.now() + timedelta(seconds=remaining_seconds)
        return None
        
    def get_throughput_mbps(self):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—ï¼ˆMB/sï¼‰"""
        elapsed = datetime.now() - self.start_time
        if elapsed.total_seconds() > 0:
            return (self.processed_bytes / 1024 / 1024) / elapsed.total_seconds()
        return 0
        
    def get_status_summary(self):
        """çŠ¶æ…‹ã‚µãƒãƒªãƒ¼å–å¾—"""
        completion_time = self.estimate_completion()
        return {
            "processed_files": self.processed_files,
            "total_files": self.total_files,
            "progress_percent": (self.processed_files / max(self.total_files, 1)) * 100,
            "processed_mb": self.processed_bytes / 1024 / 1024,
            "throughput_mbps": self.get_throughput_mbps(),
            "error_count": self.error_count,
            "success_rate": ((self.processed_files - self.error_count) / max(self.processed_files, 1)) * 100,
            "estimated_completion": completion_time.isoformat() if completion_time else None,
            "phase_times": self.phase_times
        }
```

### 3. ğŸ¥ é‹ç”¨é¢ã®æ”¹å–„

#### A. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½
**ç¾çŠ¶**: åŸºæœ¬çš„ãªãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãƒã‚§ãƒƒã‚¯ã®ã¿  
**æ”¹å–„**: åŒ…æ‹¬çš„ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç›£è¦–

```python
# æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: health_checker.py
class SystemHealthChecker:
    def __init__(self, config, auth_helper):
        self.config = config
        self.auth_helper = auth_helper
        self.logger = logging.getLogger(__name__)
        
    def check_all_systems(self):
        """å…¨ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯"""
        results = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "healthy",
            "checks": {}
        }
        
        # å„ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        checks = [
            ("api_connectivity", self.check_api_connectivity),
            ("disk_space", self.check_disk_space),
            ("memory_usage", self.check_memory_usage),
            ("network_latency", self.check_network_latency),
            ("auth_token", self.check_auth_token)
        ]
        
        for check_name, check_func in checks:
            try:
                results["checks"][check_name] = check_func()
            except Exception as e:
                results["checks"][check_name] = {
                    "status": "error",
                    "message": str(e),
                    "severity": "critical"
                }
                results["overall_status"] = "unhealthy"
                
        return results
        
    def check_api_connectivity(self):
        """Graph APIæ¥ç¶šç¢ºèª"""
        try:
            # è»½é‡ãªAPIå‘¼ã³å‡ºã—ã§ãƒ†ã‚¹ãƒˆ
            token = self.auth_helper.get_token()
            headers = {"Authorization": f"Bearer {token}"}
            
            response = requests.get(
                "https://graph.microsoft.com/v1.0/me",
                headers=headers,
                timeout=10
            )
            
            if response.status_code == 200:
                return {"status": "healthy", "response_time_ms": response.elapsed.total_seconds() * 1000}
            else:
                return {"status": "warning", "message": f"APIå¿œç­”ç•°å¸¸: {response.status_code}"}
                
        except Exception as e:
            return {"status": "critical", "message": f"APIæ¥ç¶šå¤±æ•—: {e}"}
            
    def check_disk_space(self):
        """ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç¢ºèª"""
        try:
            temp_dir = Path(self.config.get("temp_dir", "temp_downloads"))
            stat = shutil.disk_usage(temp_dir.parent)
            
            free_gb = stat.free / (1024**3)
            total_gb = stat.total / (1024**3)
            usage_percent = ((stat.total - stat.free) / stat.total) * 100
            
            status = "healthy"
            if free_gb < 5:
                status = "critical"
            elif free_gb < 20:
                status = "warning"
                
            return {
                "status": status,
                "free_gb": round(free_gb, 2),
                "total_gb": round(total_gb, 2),
                "usage_percent": round(usage_percent, 1)
            }
            
        except Exception as e:
            return {"status": "error", "message": f"ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡å–å¾—å¤±æ•—: {e}"}
            
    def check_memory_usage(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            
            status = "healthy"
            if memory.percent > 90:
                status = "critical"
            elif memory.percent > 80:
                status = "warning"
                
            return {
                "status": status,
                "usage_percent": memory.percent,
                "available_gb": round(memory.available / (1024**3), 2)
            }
            
        except ImportError:
            return {"status": "unknown", "message": "psutilãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦"}
        except Exception as e:
            return {"status": "error", "message": f"ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—å¤±æ•—: {e}"}
            
    def check_network_latency(self):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ç¢ºèª"""
        try:
            start_time = time.time()
            response = requests.get("https://graph.microsoft.com", timeout=5)
            latency_ms = (time.time() - start_time) * 1000
            
            status = "healthy"
            if latency_ms > 5000:  # 5ç§’ä»¥ä¸Š
                status = "critical"
            elif latency_ms > 2000:  # 2ç§’ä»¥ä¸Š
                status = "warning"
                
            return {
                "status": status,
                "latency_ms": round(latency_ms, 1)
            }
            
        except Exception as e:
            return {"status": "critical", "message": f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå¤±æ•—: {e}"}
            
    def check_auth_token(self):
        """èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³çŠ¶æ…‹ç¢ºèª"""
        try:
            token_info = self.auth_helper.get_token_info()
            if not token_info:
                return {"status": "critical", "message": "ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±å–å¾—å¤±æ•—"}
                
            expires_in = token_info.get("expires_in", 0)
            
            status = "healthy"
            if expires_in < 300:  # 5åˆ†æœªæº€
                status = "warning"
            elif expires_in < 60:  # 1åˆ†æœªæº€
                status = "critical"
                
            return {
                "status": status,
                "expires_in_minutes": round(expires_in / 60, 1)
            }
            
        except Exception as e:
            return {"status": "critical", "message": f"ãƒˆãƒ¼ã‚¯ãƒ³çŠ¶æ…‹ç¢ºèªå¤±æ•—: {e}"}
```

#### B. è‡ªå‹•å¾©æ—§æ©Ÿèƒ½ã®å¼·åŒ–
**ç¾çŠ¶**: åŸºæœ¬çš„ãªãƒªãƒˆãƒ©ã‚¤å‡¦ç†  
**æ”¹å–„**: ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ã®è©³ç´°å¾©æ—§æˆ¦ç•¥

```python
# æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: recovery_manager.py
class RecoveryManager:
    def __init__(self, config, auth_helper):
        self.config = config
        self.auth_helper = auth_helper
        self.logger = logging.getLogger(__name__)
        self.recovery_strategies = {
            "rate_limit": self.handle_rate_limit,
            "network_timeout": self.handle_network_timeout,
            "auth_expired": self.handle_auth_expired,
            "disk_full": self.handle_disk_full,
            "server_error": self.handle_server_error
        }
        
    def auto_recover(self, error_type, error_context, max_attempts=3):
        """ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸè‡ªå‹•å¾©æ—§"""
        if error_type not in self.recovery_strategies:
            self.logger.error(f"æœªå¯¾å¿œã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {error_type}")
            return False
            
        self.logger.info(f"è‡ªå‹•å¾©æ—§é–‹å§‹: {error_type}")
        
        for attempt in range(max_attempts):
            try:
                success = self.recovery_strategies[error_type](error_context)
                if success:
                    self.logger.info(f"è‡ªå‹•å¾©æ—§æˆåŠŸ: {error_type} (è©¦è¡Œ {attempt + 1})")
                    return True
                    
            except Exception as e:
                self.logger.error(f"å¾©æ—§å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                
            if attempt < max_attempts - 1:
                wait_time = (attempt + 1) * 30  # 30, 60, 90ç§’
                self.logger.info(f"å¾©æ—§å†è©¦è¡Œã¾ã§å¾…æ©Ÿ: {wait_time}ç§’")
                time.sleep(wait_time)
                
        self.logger.error(f"è‡ªå‹•å¾©æ—§å¤±æ•—: {error_type}")
        return False
        
    def handle_rate_limit(self, context):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ"""
        retry_after = context.get("retry_after", 60)
        self.logger.info(f"ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œå‡º: {retry_after}ç§’å¾…æ©Ÿ")
        time.sleep(retry_after)
        return True
        
    def handle_network_timeout(self, context):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾å¿œ"""
        # æ¥ç¶šæ€§ç¢ºèª
        try:
            response = requests.get("https://graph.microsoft.com", timeout=10)
            if response.status_code == 200:
                self.logger.info("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå¾©æ—§ç¢ºèª")
                return True
        except:
            self.logger.warning("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šæœªå¾©æ—§")
            
        return False
        
    def handle_auth_expired(self, context):
        """èªè¨¼æœŸé™åˆ‡ã‚Œå¯¾å¿œ"""
        try:
            # ãƒˆãƒ¼ã‚¯ãƒ³å†å–å¾—
            new_token = self.auth_helper.refresh_token()
            if new_token:
                self.logger.info("èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³æ›´æ–°æˆåŠŸ")
                return True
        except Exception as e:
            self.logger.error(f"èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³æ›´æ–°å¤±æ•—: {e}")
            
        return False
        
    def handle_disk_full(self, context):
        """ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³å¯¾å¿œ"""
        try:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            temp_dir = Path(self.config.get("temp_dir", "temp_downloads"))
            if temp_dir.exists():
                for temp_file in temp_dir.glob("*"):
                    if temp_file.is_file():
                        temp_file.unlink()
                        
            # å®¹é‡ç¢ºèª
            stat = shutil.disk_usage(temp_dir.parent)
            free_gb = stat.free / (1024**3)
            
            if free_gb > 5:  # 5GBä»¥ä¸Šç¢ºä¿ã§ããŸå ´åˆ
                self.logger.info(f"ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç¢ºä¿: {free_gb:.1f}GB")
                return True
                
        except Exception as e:
            self.logger.error(f"ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç¢ºä¿å¤±æ•—: {e}")
            
        return False
        
    def handle_server_error(self, context):
        """ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼å¯¾å¿œ"""
        # ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹ç¢ºèªç”¨ã®è»½é‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        try:
            response = requests.get("https://graph.microsoft.com/v1.0/$metadata", timeout=30)
            if response.status_code == 200:
                self.logger.info("Microsoft Graph ã‚µãƒ¼ãƒ“ã‚¹å¾©æ—§ç¢ºèª")
                return True
        except:
            self.logger.warning("Microsoft Graph ã‚µãƒ¼ãƒ“ã‚¹æœªå¾©æ—§")
            
        return False
```

### 4. ğŸ“Š ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆå¼·åŒ–

#### A. ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
**ç¾çŠ¶**: åŸºæœ¬çš„ãªçµ±è¨ˆæƒ…å ±ã®ã¿  
**æ”¹å–„**: è©³ç´°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ

```python
# æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: metrics_collector.py
class MetricsCollector:
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.metrics_file = Path("logs/metrics.json")
        
    def collect_performance_metrics(self, progress_monitor, files_info):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™åé›†"""
        throughput = progress_monitor.get_throughput_mbps()
        status = progress_monitor.get_status_summary()
        
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "performance": {
                "throughput_mbps": throughput,
                "success_rate": status["success_rate"],
                "error_rate": (status["error_count"] / max(status["processed_files"], 1)) * 100,
                "avg_file_size_mb": self.calculate_avg_file_size(files_info),
                "files_per_second": status["processed_files"] / max((datetime.now() - progress_monitor.start_time).total_seconds(), 1)
            },
            "system": {
                "phase_times": status["phase_times"],
                "total_duration_seconds": (datetime.now() - progress_monitor.start_time).total_seconds()
            },
            "quality": {
                "retry_count": self.get_retry_count(),
                "error_patterns": self.analyze_error_patterns()
            }
        }
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
        self.save_metrics(metrics)
        return metrics
        
    def calculate_avg_file_size(self, files_info):
        """å¹³å‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—"""
        if not files_info:
            return 0
            
        sizes = [f.get("size", 0) for f in files_info if isinstance(f, dict)]
        return (sum(sizes) / len(sizes)) / (1024 * 1024) if sizes else 0
        
    def get_retry_count(self):
        """ãƒªãƒˆãƒ©ã‚¤å›æ•°å–å¾—"""
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
        try:
            with open("logs/batch_main.log", "r", encoding="utf-8") as f:
                content = f.read()
                return content.count("ãƒªãƒˆãƒ©ã‚¤")
        except:
            return 0
            
    def analyze_error_patterns(self):
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        error_patterns = {}
        try:
            with open("logs/batch_error.log", "r", encoding="utf-8") as f:
                for line in f:
                    if "ERROR" in line:
                        # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã‚’æŠ½å‡ºãƒ»åˆ†é¡
                        if "timeout" in line.lower():
                            error_patterns["network_timeout"] = error_patterns.get("network_timeout", 0) + 1
                        elif "rate limit" in line.lower():
                            error_patterns["rate_limit"] = error_patterns.get("rate_limit", 0) + 1
                        elif "auth" in line.lower():
                            error_patterns["auth_error"] = error_patterns.get("auth_error", 0) + 1
                        else:
                            error_patterns["other"] = error_patterns.get("other", 0) + 1
        except:
            pass
            
        return error_patterns
        
    def save_metrics(self, metrics):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜"""
        try:
            # æ—¢å­˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿
            all_metrics = []
            if self.metrics_file.exists():
                with open(self.metrics_file, "r", encoding="utf-8") as f:
                    all_metrics = json.load(f)
                    
            # æ–°ã—ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ 
            all_metrics.append(metrics)
            
            # éå»30æ—¥åˆ†ã®ã¿ä¿æŒ
            cutoff_date = datetime.now() - timedelta(days=30)
            all_metrics = [
                m for m in all_metrics 
                if datetime.fromisoformat(m["timestamp"]) > cutoff_date
            ]
            
            # ä¿å­˜
            with open(self.metrics_file, "w", encoding="utf-8") as f:
                json.dump(all_metrics, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.logger.error(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜å¤±æ•—: {e}")
            
    def generate_performance_report(self, days=7):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            if not self.metrics_file.exists():
                return {"error": "ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“"}
                
            with open(self.metrics_file, "r", encoding="utf-8") as f:
                all_metrics = json.load(f)
                
            # éå»Næ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            cutoff_date = datetime.now() - timedelta(days=days)
            recent_metrics = [
                m for m in all_metrics
                if datetime.fromisoformat(m["timestamp"]) > cutoff_date
            ]
            
            if not recent_metrics:
                return {"error": f"éå»{days}æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“"}
                
            # çµ±è¨ˆè¨ˆç®—
            throughputs = [m["performance"]["throughput_mbps"] for m in recent_metrics]
            success_rates = [m["performance"]["success_rate"] for m in recent_metrics]
            
            report = {
                "period": f"éå»{days}æ—¥é–“",
                "executions": len(recent_metrics),
                "avg_throughput_mbps": sum(throughputs) / len(throughputs),
                "max_throughput_mbps": max(throughputs),
                "min_throughput_mbps": min(throughputs),
                "avg_success_rate": sum(success_rates) / len(success_rates),
                "trend_analysis": self.analyze_trends(recent_metrics)
            }
            
            return report
            
        except Exception as e:
            return {"error": f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå¤±æ•—: {e}"}
            
    def analyze_trends(self, metrics):
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        if len(metrics) < 2:
            return "ãƒ‡ãƒ¼ã‚¿ä¸è¶³"
            
        # ç›´è¿‘ã¨éå»ã®æ¯”è¼ƒ
        recent = metrics[-3:]  # ç›´è¿‘3å›
        older = metrics[:-3] if len(metrics) > 3 else metrics[:-1]
        
        if not older:
            return "æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ä¸è¶³"
            
        recent_avg = sum(m["performance"]["throughput_mbps"] for m in recent) / len(recent)
        older_avg = sum(m["performance"]["throughput_mbps"] for m in older) / len(older)
        
        change_percent = ((recent_avg - older_avg) / older_avg) * 100 if older_avg > 0 else 0
        
        if change_percent > 10:
            return f"æ€§èƒ½å‘ä¸Šå‚¾å‘ (+{change_percent:.1f}%)"
        elif change_percent < -10:
            return f"æ€§èƒ½ä½ä¸‹å‚¾å‘ ({change_percent:.1f}%)"
        else:
            return "æ€§èƒ½å®‰å®š"
```

#### B. äºˆæ¸¬ã‚¢ãƒ©ãƒ¼ãƒˆ
**ç¾çŠ¶**: äº‹å¾Œçš„ãªã‚¨ãƒ©ãƒ¼é€šçŸ¥ã®ã¿  
**æ”¹å–„**: äºˆæ¸¬å‹ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»æ—©æœŸè­¦å‘Š

```python
# æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: predictive_alerts.py
class PredictiveAlerts:
    def __init__(self, config, metrics_collector):
        self.config = config
        self.metrics_collector = metrics_collector
        self.logger = logging.getLogger(__name__)
        
    def check_completion_delay_risk(self, progress_monitor, target_completion_time):
        """å®Œäº†é…å»¶ãƒªã‚¹ã‚¯äºˆæ¸¬"""
        estimated_completion = progress_monitor.estimate_completion()
        if not estimated_completion:
            return None
            
        if estimated_completion > target_completion_time:
            delay_hours = (estimated_completion - target_completion_time).total_seconds() / 3600
            return {
                "risk_level": "high" if delay_hours > 2 else "medium",
                "estimated_delay_hours": delay_hours,
                "recommended_action": "ä¸¦åˆ—åº¦ã‚’ä¸Šã’ã‚‹ã‹ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã¸ã®åˆ‡ã‚Šæ›¿ãˆã‚’æ¤œè¨"
            }
            
        return {"risk_level": "low"}
        
    def check_disk_shortage_risk(self, remaining_files, avg_file_size):
        """ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³ãƒªã‚¹ã‚¯äºˆæ¸¬"""
        try:
            temp_dir = Path(self.config.get("temp_dir", "temp_downloads"))
            stat = shutil.disk_usage(temp_dir.parent)
            free_gb = stat.free / (1024**3)
            
            # æ®‹ã‚Šãƒ•ã‚¡ã‚¤ãƒ«ã®æ¨å®šå®¹é‡
            estimated_need_gb = (remaining_files * avg_file_size) / (1024**3)
            
            # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ï¼ˆ2å€ï¼‰ã‚’è€ƒæ…®
            risk_threshold_gb = estimated_need_gb * 2
            
            if free_gb < risk_threshold_gb:
                return {
                    "risk_level": "high",
                    "free_gb": free_gb,
                    "estimated_need_gb": estimated_need_gb,
                    "recommended_action": "ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã¾ãŸã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã¸ã®åˆ‡ã‚Šæ›¿ãˆ"
                }
            elif free_gb < risk_threshold_gb * 1.5:
                return {
                    "risk_level": "medium",
                    "free_gb": free_gb,
                    "estimated_need_gb": estimated_need_gb,
                    "recommended_action": "ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç›£è¦–ã‚’å¼·åŒ–"
                }
                
            return {"risk_level": "low"}
            
        except Exception as e:
            return {"risk_level": "unknown", "error": str(e)}
            
    def check_performance_degradation(self):
        """æ€§èƒ½åŠ£åŒ–äºˆæ¸¬"""
        try:
            report = self.metrics_collector.generate_performance_report(days=3)
            if "error" in report:
                return {"risk_level": "unknown", "message": report["error"]}
                
            # éå»ãƒ‡ãƒ¼ã‚¿ã¨ã®æ¯”è¼ƒ
            if "æ€§èƒ½ä½ä¸‹" in report.get("trend_analysis", ""):
                return {
                    "risk_level": "medium",
                    "current_avg_mbps": report["avg_throughput_mbps"],
                    "recommended_action": "ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çŠ¶æ³ã®ç¢ºèª"
                }
                
            return {"risk_level": "low"}
            
        except Exception as e:
            return {"risk_level": "unknown", "error": str(e)}
            
    def generate_alert_summary(self, progress_monitor, target_completion_time):
        """ç·åˆã‚¢ãƒ©ãƒ¼ãƒˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        alerts = {
            "timestamp": datetime.now().isoformat(),
            "overall_risk": "low",
            "alerts": []
        }
        
        # å„ç¨®ãƒªã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯
        status = progress_monitor.get_status_summary()
        remaining_files = status["total_files"] - status["processed_files"]
        avg_file_size = status.get("processed_mb", 0) * 1024 * 1024 / max(status["processed_files"], 1)
        
        risk_checks = [
            ("completion_delay", self.check_completion_delay_risk(progress_monitor, target_completion_time)),
            ("disk_shortage", self.check_disk_shortage_risk(remaining_files, avg_file_size)),
            ("performance_degradation", self.check_performance_degradation())
        ]
        
        for check_name, result in risk_checks:
            if result and result.get("risk_level") in ["medium", "high"]:
                alerts["alerts"].append({
                    "type": check_name,
                    "risk_level": result["risk_level"],
                    "details": result
                })
                
                # å…¨ä½“ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«æ›´æ–°
                if result["risk_level"] == "high":
                    alerts["overall_risk"] = "high"
                elif result["risk_level"] == "medium" and alerts["overall_risk"] == "low":
                    alerts["overall_risk"] = "medium"
                    
        return alerts
```

### 5. ğŸ§ª ãƒ†ã‚¹ãƒˆå¼·åŒ–

#### A. çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
**ç¾çŠ¶**: å€‹åˆ¥æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆã®ã¿  
**æ”¹å–„**: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ»è² è·ãƒ»éšœå®³å¾©æ—§ãƒ†ã‚¹ãƒˆ

```python
# æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: integration_tests.py
class IntegrationTestSuite:
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.test_results = []
        
    def run_all_tests(self):
        """å…¨çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        tests = [
            ("auth_flow", self.test_authentication_flow),
            ("small_batch", self.test_small_batch_cycle),
            ("large_file", self.test_large_file_handling),
            ("error_recovery", self.test_error_recovery),
            ("concurrent_execution", self.test_concurrent_execution),
            ("resource_cleanup", self.test_resource_cleanup)
        ]
        
        results = {"timestamp": datetime.now().isoformat(), "tests": {}}
        
        for test_name, test_func in tests:
            self.logger.info(f"çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: {test_name}")
            try:
                result = test_func()
                results["tests"][test_name] = {
                    "status": "passed" if result else "failed",
                    "details": result if isinstance(result, dict) else {}
                }
            except Exception as e:
                results["tests"][test_name] = {
                    "status": "error",
                    "error": str(e)
                }
                
        # çµæœä¿å­˜
        self.save_test_results(results)
        return results
        
    def test_authentication_flow(self):
        """èªè¨¼ãƒ•ãƒ­ãƒ¼å®Œå…¨ãƒ†ã‚¹ãƒˆ"""
        try:
            # 1. ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
            required_env = ["CLIENT_ID", "TENANT_ID", "CLIENT_SECRET"]
            for env in required_env:
                if not os.getenv(env):
                    return {"error": f"ç’°å¢ƒå¤‰æ•°æœªè¨­å®š: {env}"}
                    
            # 2. ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—
            auth_helper = AuthHelper(self.config)
            token = auth_helper.get_token()
            if not token:
                return {"error": "ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å¤±æ•—"}
                
            # 3. APIå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ
            headers = {"Authorization": f"Bearer {token}"}
            response = requests.get("https://graph.microsoft.com/v1.0/me", headers=headers)
            
            if response.status_code != 200:
                return {"error": f"APIå‘¼ã³å‡ºã—å¤±æ•—: {response.status_code}"}
                
            return True
            
        except Exception as e:
            return {"error": str(e)}
            
    def test_small_batch_cycle(self):
        """å°è¦æ¨¡ãƒãƒƒãƒã‚µã‚¤ã‚¯ãƒ«ãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆç”¨ã®å°ã•ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚»ãƒƒãƒˆã§å®Œå…¨ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œ
        # å®Ÿè£…ã¯å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã«ä¾å­˜
        pass
        
    def test_large_file_handling(self):
        """å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        # å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒ£ãƒ³ã‚¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ
        pass
        
    def test_error_recovery(self):
        """ã‚¨ãƒ©ãƒ¼å¾©æ—§æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
        # æ„å›³çš„ãªã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã¨å¾©æ—§å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ
        pass
        
    def test_concurrent_execution(self):
        """ä¸¦è¡Œå®Ÿè¡Œåˆ¶å¾¡ãƒ†ã‚¹ãƒˆ"""
        # æ’ä»–åˆ¶å¾¡ã®å‹•ä½œç¢ºèª
        pass
        
    def test_resource_cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹æ¸…ç†ãƒ†ã‚¹ãƒˆ"""
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ¸…ç†ã®ç¢ºèª
        pass
        
    def save_test_results(self, results):
        """ãƒ†ã‚¹ãƒˆçµæœä¿å­˜"""
        test_results_file = Path("logs/integration_test_results.json")
        try:
            with open(test_results_file, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ãƒ†ã‚¹ãƒˆçµæœä¿å­˜å¤±æ•—: {e}")
```

---

## ğŸ” ISSUES.mdåˆ†æçµæœã¨ã®çµ±åˆ

### âœ… **è§£æ±ºæ¸ˆã¿é …ç›®ã®ç¢ºèª**

ISSUES.mdã§å ±å‘Šã•ã‚ŒãŸé‡è¦åº¦ã®é«˜ã„å•é¡Œã¯å…¨ã¦è§£æ±ºæ¸ˆã¿ã§ã™ï¼š

1. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯** âœ… - ç’°å¢ƒå¤‰æ•°ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…æ¸ˆã¿
2. **é‡è¤‡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«** âœ… - æ–°v2.0ã‚·ã‚¹ãƒ†ãƒ ã§çµ±ä¸€æ¸ˆã¿  
3. **ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼** âœ… - æ–°å®Ÿè£…ã§æ•´ç†å®Œäº†
4. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸çµ±ä¸€** âœ… - `graph_api_helper.py`ã§çµ±ä¸€æ¸ˆã¿
5. **ä¸¦è¡Œå‡¦ç†ç«¶åˆ** âœ… - `log_manager.py`ã§æ’ä»–åˆ¶å¾¡å®Ÿè£…æ¸ˆã¿
6. **è¨­å®šæ¤œè¨¼ä¸è¶³** âœ… - `batch_config.py`ã§æ¤œè¨¼æ©Ÿèƒ½å®Ÿè£…æ¸ˆã¿

### ğŸ“ **æ®‹å­˜æ”¹å–„é …ç›®ã®çµ±åˆ**

ISSUES.mdã§ã€Œæ”¹å–„æ¨å¥¨ã€ã¨ã•ã‚ŒãŸé …ç›®ã‚’æœ¬æ”¹å–„è¨ˆç”»ã«çµ±åˆã—ã¾ã™ï¼š

#### 6. ğŸ§¹ ã‚³ãƒ¼ãƒ‰å“è³ªã®ç´°éƒ¨æ”¹å–„

##### C. ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®çµ±ä¸€åŒ–
**ç¾çŠ¶**: printæ–‡ã¨loggingæ··åœ¨ã«ã‚ˆã‚‹é‡è¦åº¦ã®ä¸æ˜ç¢ºã•  
**æ”¹å–„**: çµ±ä¸€ã•ã‚ŒãŸãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ä½“ç³»ã®å°å…¥

```python
# æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: logging_standards.py
class LoggingStandards:
    """çµ±ä¸€ãƒ­ã‚°æ¨™æº–"""
    
    @staticmethod
    def setup_standard_logging():
        """æ¨™æº–ãƒ­ã‚°è¨­å®š"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/batch_main.log'),
                logging.StreamHandler()
            ]
        )
    
    @staticmethod
    def replace_print_statements():
        """printæ–‡ã®æ®µéšçš„ç½®æ›ã‚¬ã‚¤ãƒ‰"""
        # æ®µéš1: æƒ…å ±å‡ºåŠ›ã®åˆ†é¡
        # print("ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ") â†’ logging.info("ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ")
        # print("âŒ ä½œæˆå¤±æ•—") â†’ logging.error("âŒ ä½œæˆå¤±æ•—")
        # print("ğŸŒ URL:") â†’ logging.debug("ğŸŒ URL:")
        pass

# æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®é©ç”¨ä¾‹
class LogMigrationHelper:
    """ãƒ­ã‚°ç§»è¡Œãƒ˜ãƒ«ãƒ‘ãƒ¼"""
    
    LOG_LEVEL_MAPPING = {
        "ğŸ“": logging.INFO,    # ãƒ•ã‚©ãƒ«ãƒ€æ“ä½œ
        "âŒ": logging.ERROR,   # ã‚¨ãƒ©ãƒ¼
        "âœ…": logging.INFO,    # æˆåŠŸ
        "ğŸŒ": logging.DEBUG,   # URLãƒ»è©³ç´°æƒ…å ±
        "âš ï¸": logging.WARNING, # è­¦å‘Š
        "ğŸ”„": logging.INFO,    # å‡¦ç†ä¸­
    }
    
    @classmethod
    def convert_print_to_logging(cls, message):
        """çµµæ–‡å­—ãƒ™ãƒ¼ã‚¹ã§printæ–‡ã‚’ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ¤å®š"""
        for emoji, level in cls.LOG_LEVEL_MAPPING.items():
            if message.startswith(emoji):
                return level
        return logging.INFO  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
```

**å®Ÿè£…å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«**:
- `sharepoint_uploader.py` - ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†
- `file_downloader.py` - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—è¡¨ç¤º
- `onedrive_crawler.py` - ã‚¯ãƒ­ãƒ¼ãƒ«é€²æ—è¡¨ç¤º
- å„ç¨®æ—§ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆVer0.1ãƒ•ã‚©ãƒ«ãƒ€å†…ï¼‰

##### D. ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å€¤ã®å®šæ•°åŒ–
**ç¾çŠ¶**: ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ãƒ»è¨­å®šå€¤ã®ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿  
**æ”¹å–„**: å®šæ•°ã‚¯ãƒ©ã‚¹ãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«åŒ–

```python
# æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: constants.py
class SystemConstants:
    """ã‚·ã‚¹ãƒ†ãƒ å®šæ•°å®šç¾©"""
    
    # èªè¨¼é–¢é€£
    TOKEN_EXPIRE_MARGIN_SECONDS = 300  # 5åˆ†ã®ãƒãƒ¼ã‚¸ãƒ³
    DEFAULT_TOKEN_LIFETIME_SECONDS = 3600  # 1æ™‚é–“
    
    # APIåˆ¶é™
    GRAPH_API_RATE_LIMIT_DELAY = 60  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ™‚ã®å¾…æ©Ÿç§’æ•°
    MAX_RETRY_ATTEMPTS = 5
    RETRY_BACKOFF_FACTOR = 2
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
    CHUNK_SIZE_THRESHOLDS = {
        "small": 1024 * 1024,      # 1MBæœªæº€ã¯å°ãƒ•ã‚¡ã‚¤ãƒ«
        "medium": 10 * 1024 * 1024, # 10MBæœªæº€ã¯ä¸­ãƒ•ã‚¡ã‚¤ãƒ«
        "large": 100 * 1024 * 1024  # 100MBä»¥ä¸Šã¯å¤§ãƒ•ã‚¡ã‚¤ãƒ«
    }
    
    # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡
    DISK_WARNING_THRESHOLD_GB = 20
    DISK_CRITICAL_THRESHOLD_GB = 5
    
    # å‡¦ç†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    DEFAULT_REQUEST_TIMEOUT = 30
    LARGE_FILE_UPLOAD_TIMEOUT = 300
    
    # ãƒ­ã‚°ä¿æŒ
    LOG_RETENTION_DAYS = 30
    METRICS_RETENTION_DAYS = 90

# è¨­å®šã®å‹•çš„èª­ã¿è¾¼ã¿å¯¾å¿œ
class ConfigurableConstants:
    """è¨­å®šå¯èƒ½å®šæ•°ï¼ˆconfig.jsonã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰"""
    
    def __init__(self, config):
        self.config = config
        
    @property
    def token_expire_margin(self):
        """ãƒˆãƒ¼ã‚¯ãƒ³æœ‰åŠ¹æœŸé™ãƒãƒ¼ã‚¸ãƒ³ï¼ˆç§’ï¼‰"""
        return self.config.get("auth_settings", {}).get("expire_margin", 300)
        
    @property
    def max_chunk_size_bytes(self):
        """æœ€å¤§ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆï¼‰"""
        mb_size = self.config.get("max_chunk_size_mb", 4)
        return mb_size * 1024 * 1024
        
    @property
    def performance_thresholds(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é–¾å€¤"""
        return {
            "slow_throughput_mbps": self.config.get("performance", {}).get("slow_threshold", 1.0),
            "fast_throughput_mbps": self.config.get("performance", {}).get("fast_threshold", 10.0)
        }
```

**å®Ÿè£…å¯¾è±¡ç®‡æ‰€**:
```python
# ä¿®æ­£å‰ï¼ˆauth_helper.pyï¼‰
_token_info["expires_at"] = now + timedelta(seconds=3300)

# ä¿®æ­£å¾Œ
from constants import SystemConstants
expires_in = result.get("expires_in", SystemConstants.DEFAULT_TOKEN_LIFETIME_SECONDS)
_token_info["expires_at"] = now + timedelta(
    seconds=expires_in - SystemConstants.TOKEN_EXPIRE_MARGIN_SECONDS
)
```

#### 7. ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ”¹å–„

##### A. è²¬å‹™åˆ†é›¢ã®å¼·åŒ–
**ç¾çŠ¶**: å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å†…ã§ã®è¤‡æ•°è²¬å‹™æ··åœ¨  
**æ”¹å–„**: æ˜ç¢ºãªè²¬å‹™å¢ƒç•Œã¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è¨­è¨ˆ

```python
# æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: batch_orchestrator.py
class BatchOrchestrator:
    """ãƒãƒƒãƒå‡¦ç†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆè²¬å‹™åˆ†é›¢ç‰ˆï¼‰"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # å„ãƒ•ã‚§ãƒ¼ã‚ºã®å°‚ç”¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        self.crawler = OneDriveCrawler(config)
        self.downloader = FileDownloader(config)
        self.uploader = SharePointUploader(config)
        self.monitor = ProgressMonitor()
        
    def execute_batch_pipeline(self):
        """è²¬å‹™åˆ†é›¢ã•ã‚ŒãŸãƒãƒƒãƒãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        try:
            # ãƒ•ã‚§ãƒ¼ã‚º1: æƒ…å ±åé›†
            self.monitor.start_phase("crawl")
            files_metadata = self.crawler.discover_files()
            self.monitor.end_phase("crawl")
            
            # ãƒ•ã‚§ãƒ¼ã‚º2: å‡¦ç†è¨ˆç”»
            self.monitor.start_phase("planning")
            execution_plan = self.create_execution_plan(files_metadata)
            self.monitor.end_phase("planning")
            
            # ãƒ•ã‚§ãƒ¼ã‚º3: ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
            if execution_plan.requires_download:
                self.monitor.start_phase("download")
                self.downloader.execute_download_plan(execution_plan.download_list)
                self.monitor.end_phase("download")
            
            # ãƒ•ã‚§ãƒ¼ã‚º4: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            self.monitor.start_phase("upload")
            self.uploader.execute_upload_plan(execution_plan.upload_list)
            self.monitor.end_phase("upload")
            
            # ãƒ•ã‚§ãƒ¼ã‚º5: å¾Œå‡¦ç†
            self.monitor.start_phase("cleanup")
            self.cleanup_resources(execution_plan)
            self.monitor.end_phase("cleanup")
            
        except Exception as e:
            self.handle_pipeline_error(e)
            
    def create_execution_plan(self, files_metadata):
        """ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹æ€§ã«åŸºã¥ã„ãŸå®Ÿè¡Œè¨ˆç”»ä½œæˆ"""
        plan = ExecutionPlan()
        
        for file_info in files_metadata:
            if self.should_use_streaming(file_info):
                plan.streaming_list.append(file_info)
            elif self.requires_chunked_upload(file_info):
                plan.chunked_upload_list.append(file_info)
            else:
                plan.standard_upload_list.append(file_info)
                
        return plan

class ExecutionPlan:
    """å®Ÿè¡Œè¨ˆç”»ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        self.streaming_list = []
        self.chunked_upload_list = []
        self.standard_upload_list = []
        self.requires_download = True
        
    @property
    def download_list(self):
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§"""
        return [f for f in self.all_files if not self.is_streaming_file(f)]
        
    @property
    def upload_list(self):
        """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§"""
        return self.streaming_list + self.chunked_upload_list + self.standard_upload_list
```

##### B. ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£ã®å‘ä¸Š
**ç¾çŠ¶**: å¤–éƒ¨APIä¾å­˜ã«ã‚ˆã‚‹å˜ä½“ãƒ†ã‚¹ãƒˆå›°é›£  
**æ”¹å–„**: ä¾å­˜æ€§æ³¨å…¥ãƒ»ãƒ¢ãƒƒã‚¯å¯¾å¿œè¨­è¨ˆ

```python
# æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: interfaces.py
from abc import ABC, abstractmethod

class GraphAPIInterface(ABC):
    """Graph APIæŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    
    @abstractmethod
    def make_request(self, method, url, **kwargs):
        """APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        pass
        
    @abstractmethod
    def download_file(self, file_id, local_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        pass
        
    @abstractmethod
    def upload_file(self, local_path, remote_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        pass

class ConfigInterface(ABC):
    """è¨­å®šç®¡ç†æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    
    @abstractmethod
    def get(self, key, default=None):
        """è¨­å®šå€¤å–å¾—"""
        pass
        
    @abstractmethod
    def validate(self):
        """è¨­å®šå€¤æ¤œè¨¼"""
        pass

# ãƒ†ã‚¹ãƒˆå¯¾å¿œå®Ÿè£…
class TestableFileDownloader:
    """ãƒ†ã‚¹ãƒˆå¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, config: ConfigInterface, api_client: GraphAPIInterface):
        self.config = config
        self.api_client = api_client
        self.logger = logging.getLogger(__name__)
        
    def download_file(self, file_info):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ†ã‚¹ãƒˆå¯èƒ½ç‰ˆï¼‰"""
        try:
            local_path = self.build_local_path(file_info)
            success = self.api_client.download_file(file_info["id"], local_path)
            
            if success:
                self.logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {file_info['name']}")
                return True
            else:
                self.logger.error(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {file_info['name']}")
                return False
                
        except Exception as e:
            self.logger.error(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
            
    def build_local_path(self, file_info):
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹æ§‹ç¯‰ï¼ˆãƒ†ã‚¹ãƒˆå¯èƒ½ï¼‰"""
        temp_dir = self.config.get("temp_dir", "temp_downloads")
        return Path(temp_dir) / file_info["relative_path"]

# ãƒ¢ãƒƒã‚¯å®Ÿè£…ä¾‹
class MockGraphAPIClient(GraphAPIInterface):
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒƒã‚¯Graph APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.requests_made = []
        self.responses = {}
        
    def make_request(self, method, url, **kwargs):
        """ãƒ¢ãƒƒã‚¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        self.requests_made.append({"method": method, "url": url, "kwargs": kwargs})
        return self.responses.get(url, {"status_code": 200, "json": {}})
        
    def download_file(self, file_id, local_path):
        """ãƒ¢ãƒƒã‚¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        Path(local_path).parent.mkdir(parents=True, exist_ok=True)
        Path(local_path).write_text("test content")
        return True
        
    def upload_file(self, local_path, remote_path):
        """ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        return True
```

### ğŸ“Š **çµ±åˆå®Ÿè£…è¨ˆç”»ã®æ›´æ–°**

ISSUES.mdã®åˆ†æçµæœã‚’è¸ã¾ãˆã€å®Ÿè£…è¨ˆç”»ã‚’æ›´æ–°ã—ã¾ã™ï¼š

#### ãƒ•ã‚§ãƒ¼ã‚º1: åŸºç›¤å¼·åŒ–ï¼ˆæ¨å®š2.5é€±é–“ï¼‰
**å„ªå…ˆåº¦: ğŸ”´ é«˜**

| ã‚¿ã‚¹ã‚¯ | å·¥æ•° | æ‹…å½“ãƒ•ã‚¡ã‚¤ãƒ« | åŠ¹æœ | æ›´æ–° |
|--------|------|-------------|------|------|
| ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ¨™æº–åŒ– | 3æ—¥ | `exceptions.py`ï¼ˆæ–°è¦ï¼‰<br/>`auth_helper.py`<br/>`graph_api_helper.py` | å®‰å®šæ€§å‘ä¸Š | - |
| ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ç›£è¦–å¼·åŒ– | 2æ—¥ | `progress_monitor.py`ï¼ˆæ–°è¦ï¼‰<br/>`main_batch.py` | é‹ç”¨æ€§å‘ä¸Š | - |
| è¨­å®šæ¤œè¨¼å¼·åŒ– | 1æ—¥ | `batch_config.py` | å“è³ªå‘ä¸Š | - |
| ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ | 3æ—¥ | `health_checker.py`ï¼ˆæ–°è¦ï¼‰ | é‹ç”¨æ€§å‘ä¸Š | - |
| **ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«çµ±ä¸€åŒ–** | **1.5æ—¥** | **`logging_standards.py`ï¼ˆæ–°è¦ï¼‰<br/>æ—¢å­˜å…¨ãƒ•ã‚¡ã‚¤ãƒ«** | **ä¿å®ˆæ€§å‘ä¸Š** | **æ–°è¦** |
| **å®šæ•°åŒ–ãƒ»ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è§£æ¶ˆ** | **1æ—¥** | **`constants.py`ï¼ˆæ–°è¦ï¼‰<br/>`auth_helper.py`** | **ä¿å®ˆæ€§å‘ä¸Š** | **æ–°è¦** |

#### ãƒ•ã‚§ãƒ¼ã‚º2: æœ€é©åŒ–ãƒ»ç›£è¦–ï¼ˆæ¨å®š2.5é€±é–“ï¼‰
**å„ªå…ˆåº¦: ğŸŸ¡ ä¸­**

| ã‚¿ã‚¹ã‚¯ | å·¥æ•° | æ‹…å½“ãƒ•ã‚¡ã‚¤ãƒ« | åŠ¹æœ | æ›´æ–° |
|--------|------|-------------|------|------|
| å‹•çš„ä¸¦åˆ—åº¦èª¿æ•´ | 4æ—¥ | `performance_optimizer.py`ï¼ˆæ–°è¦ï¼‰<br/>`main_batch.py` | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š | - |
| è‡ªå‹•å¾©æ—§æ©Ÿèƒ½å¼·åŒ– | 3æ—¥ | `recovery_manager.py`ï¼ˆæ–°è¦ï¼‰ | å®‰å®šæ€§å‘ä¸Š | - |
| ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›† | 3æ—¥ | `metrics_collector.py`ï¼ˆæ–°è¦ï¼‰ | ç›£è¦–å¼·åŒ– | - |
| **è²¬å‹™åˆ†é›¢å¼·åŒ–** | **2æ—¥** | **`batch_orchestrator.py`ï¼ˆæ–°è¦ï¼‰<br/>`main_batch.py`** | **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„** | **æ–°è¦** |

#### ãƒ•ã‚§ãƒ¼ã‚º3: é«˜åº¦æ©Ÿèƒ½ï¼ˆæ¨å®š1.5é€±é–“ï¼‰
**å„ªå…ˆåº¦: ğŸŸ¢ ä½**

| ã‚¿ã‚¹ã‚¯ | å·¥æ•° | æ‹…å½“ãƒ•ã‚¡ã‚¤ãƒ« | åŠ¹æœ | æ›´æ–° |
|--------|------|-------------|------|------|
| äºˆæ¸¬ã‚¢ãƒ©ãƒ¼ãƒˆ | 4æ—¥ | `predictive_alerts.py`ï¼ˆæ–°è¦ï¼‰ | äºˆé˜²çš„é‹ç”¨ | - |
| **ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£å‘ä¸Š** | **3æ—¥** | **`interfaces.py`ï¼ˆæ–°è¦ï¼‰<br/>æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç¾¤** | **å“è³ªä¿è¨¼ãƒ»ä¿å®ˆæ€§** | **æ–°è¦** |
| çµ±åˆãƒ†ã‚¹ãƒˆ | 3æ—¥ | `integration_tests.py`ï¼ˆæ–°è¦ï¼‰ | å“è³ªä¿è¨¼ | çµ±åˆãƒ†ã‚¹ãƒˆã«ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£æ”¹å–„ã‚’å«ã‚€ |

### ğŸ’¾ **æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼ˆæ›´æ–°ç‰ˆï¼‰**

| ãƒ•ã‚¡ã‚¤ãƒ«å | ç›®çš„ | ä¸»è¦æ©Ÿèƒ½ | æ›´æ–° |
|-----------|------|----------|------|
| `exceptions.py` | çµ±ä¸€ä¾‹å¤–ã‚¯ãƒ©ã‚¹ | ã‚¨ãƒ©ãƒ¼åˆ†é¡ãƒ»æ¨™æº–åŒ– | - |
| `performance_optimizer.py` | æ€§èƒ½æœ€é©åŒ– | å‹•çš„ä¸¦åˆ—åº¦èª¿æ•´ãƒ»ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºæœ€é©åŒ– | - |
| `progress_monitor.py` | é€²æ—ç›£è¦– | è©³ç´°é€²æ—ãƒ»å®Œäº†äºˆæ¸¬ãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ | - |
| `health_checker.py` | ã‚·ã‚¹ãƒ†ãƒ ç›£è¦– | APIãƒ»ãƒ‡ã‚£ã‚¹ã‚¯ãƒ»ãƒ¡ãƒ¢ãƒªãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç›£è¦– | - |
| `recovery_manager.py` | è‡ªå‹•å¾©æ—§ | ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥å¾©æ—§æˆ¦ç•¥ | - |
| `metrics_collector.py` | ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›† | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ | - |
| `predictive_alerts.py` | äºˆæ¸¬ã‚¢ãƒ©ãƒ¼ãƒˆ | å®Œäº†é…å»¶ãƒ»å®¹é‡ä¸è¶³ãƒ»æ€§èƒ½åŠ£åŒ–äºˆæ¸¬ | - |
| `integration_tests.py` | çµ±åˆãƒ†ã‚¹ãƒˆ | ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ»è² è·ãƒ»å¾©æ—§ãƒ†ã‚¹ãƒˆ | - |
| **`logging_standards.py`** | **ãƒ­ã‚°æ¨™æº–åŒ–** | **çµ±ä¸€ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ãƒ»printæ–‡ç½®æ›** | **æ–°è¦** |
| **`constants.py`** | **å®šæ•°ç®¡ç†** | **ã‚·ã‚¹ãƒ†ãƒ å®šæ•°ãƒ»è¨­å®šå€¤å®šæ•°åŒ–** | **æ–°è¦** |
| **`batch_orchestrator.py`** | **ãƒãƒƒãƒçµ±åˆ¶** | **è²¬å‹™åˆ†é›¢ãƒ»ãƒ•ã‚§ãƒ¼ã‚ºç®¡ç†** | **æ–°è¦** |
| **`interfaces.py`** | **æŠ½è±¡åŒ–** | **ãƒ†ã‚¹ãƒˆç”¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãƒ»ãƒ¢ãƒƒã‚¯å¯¾å¿œ** | **æ–°è¦** |

### ğŸ”„ **ç¶™ç¶šçš„æ”¹å–„è¨ˆç”»ã®å¼·åŒ–**

ISSUES.mdã®ç¶™ç¶šçš„æ”¹å–„ææ¡ˆã‚’çµ±åˆï¼š

#### é–‹ç™ºãƒ—ãƒ­ã‚»ã‚¹æ”¹å–„
- **ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹**: ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¿…é ˆãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
- **è‡ªå‹•ãƒ†ã‚¹ãƒˆå®Ÿè£…**: pytest ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ¢ãƒƒã‚¯å¯¾å¿œ
- **å®šæœŸã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»**: æ©Ÿå¯†æƒ…å ±ãƒ»ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªè„†å¼±æ€§ãƒã‚§ãƒƒã‚¯

#### å“è³ªç®¡ç†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
```python
# æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: quality_checklist.py
class QualityChecklist:
    """å“è³ªç®¡ç†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ"""
    
    SECURITY_CHECKS = [
        "æ©Ÿå¯†æƒ…å ±ã®ç’°å¢ƒå¤‰æ•°åŒ–ç¢ºèª",
        "ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸèªè¨¼æƒ…å ±ã®æ’é™¤",
        "ãƒ­ã‚°å‡ºåŠ›ã«ãŠã‘ã‚‹æ©Ÿå¯†æƒ…å ±ãƒã‚¹ã‚­ãƒ³ã‚°",
        "ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è„†å¼±æ€§ãƒã‚§ãƒƒã‚¯"
    ]
    
    CODE_QUALITY_CHECKS = [
        "printæ–‡ã®loggingç½®æ›",
        "ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã®å®šæ•°åŒ–",
        "ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®çµ±ä¸€",
        "è²¬å‹™åˆ†é›¢ã®ç¢ºèª",
        "ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸80%ä»¥ä¸Š"
    ]
    
    PERFORMANCE_CHECKS = [
        "ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ç¢ºèª",
        "ä¸¦åˆ—å‡¦ç†ã®ç«¶åˆçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯",
        "ãƒªã‚½ãƒ¼ã‚¹é©åˆ‡è§£æ”¾ã®ç¢ºèª",
        "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã®å¦¥å½“æ€§"
    ]
```

---

## ğŸ“ˆ **æœŸå¾…åŠ¹æœï¼ˆæ›´æ–°ç‰ˆï¼‰**

### å®šé‡çš„åŠ¹æœ
- **ã‚¨ãƒ©ãƒ¼ç‡å‰Šæ¸›**: ç¾åœ¨ã®5%ã‹ã‚‰2%ä»¥ä¸‹
- **å‡¦ç†é€Ÿåº¦å‘ä¸Š**: 20-30%ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ”¹å–„
- **å¾©æ—§æ™‚é–“çŸ­ç¸®**: æ‰‹å‹•ä»‹å…¥ã‹ã‚‰è‡ªå‹•å¾©æ—§ã¸
- **ç›£è¦–ç²¾åº¦å‘ä¸Š**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŠ¶æ³æŠŠæ¡
- **ä¿å®ˆå·¥æ•°å‰Šæ¸›**: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«çµ±ä¸€ãƒ»å®šæ•°åŒ–ã«ã‚ˆã‚Š20%å‰Šæ¸›ï¼ˆæ–°è¦ï¼‰
- **ãƒ†ã‚¹ãƒˆå·¥æ•°å‰Šæ¸›**: ãƒ¢ãƒƒã‚¯å¯¾å¿œã«ã‚ˆã‚Šå˜ä½“ãƒ†ã‚¹ãƒˆæ™‚é–“50%çŸ­ç¸®ï¼ˆæ–°è¦ï¼‰

### å®šæ€§çš„åŠ¹æœ
- **é‹ç”¨è² è·è»½æ¸›**: è‡ªå‹•åŒ–ãƒ»äºˆæ¸¬æ©Ÿèƒ½ã«ã‚ˆã‚‹ç®¡ç†å·¥æ•°å‰Šæ¸›
- **å“è³ªå‘ä¸Š**: çµ±åˆãƒ†ã‚¹ãƒˆãƒ»æ¨™æº–åŒ–ã«ã‚ˆã‚‹ä¿¡é ¼æ€§å‘ä¸Š
- **æ‹¡å¼µæ€§ç¢ºä¿**: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆã«ã‚ˆã‚‹æ©Ÿèƒ½è¿½åŠ å®¹æ˜“æ€§
- **ä¿å®ˆæ€§å‘ä¸Š**: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ãƒ­ã‚°ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ã‚ˆã‚‹è¿½è·¡å¯èƒ½æ€§
- **é–‹ç™ºåŠ¹ç‡å‘ä¸Š**: è²¬å‹™åˆ†é›¢ãƒ»ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹åŒ–ã«ã‚ˆã‚‹ä¸¦è¡Œé–‹ç™ºå¯èƒ½ï¼ˆæ–°è¦ï¼‰
- **å•é¡Œç™ºè¦‹è¿…é€ŸåŒ–**: çµ±ä¸€ãƒ­ã‚°ãƒ»å®šæ•°åŒ–ã«ã‚ˆã‚‹å•é¡Œç®‡æ‰€ç‰¹å®šæ™‚é–“çŸ­ç¸®ï¼ˆæ–°è¦ï¼‰

**ğŸ“ æ³¨æ„**: ã“ã®çµ±åˆæ”¹å–„è¨ˆç”»ã¯ã€ISSUES.mdã§ç‰¹å®šã•ã‚ŒãŸæ—¢å­˜å•é¡Œã®è§£æ±ºçŠ¶æ³ã‚’ç¢ºèªã—ã€æ®‹å­˜ã™ã‚‹æ”¹å–„æ¨å¥¨é …ç›®ã‚’ä½“ç³»çš„ã«çµ„ã¿è¾¼ã‚“ã ã‚‚ã®ã§ã™ã€‚æ—¢ã«å®‰å®šç¨¼åƒã—ã¦ã„ã‚‹ç¾ã‚·ã‚¹ãƒ†ãƒ ã®åŸºç›¤ã®ä¸Šã«ã€ã•ã‚‰ãªã‚‹å“è³ªå‘ä¸Šã‚’å›³ã‚‹å†…å®¹ã¨ãªã£ã¦ã„ã¾ã™ã€‚
